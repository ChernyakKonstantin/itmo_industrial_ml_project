{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    def __init__(self, point_hash: str):\n",
    "        self.point_hash = point_hash\n",
    "        self.result_per_day = None\n",
    "        self.result_per_hour = None\n",
    "        self.resid_regr = None\n",
    "        self.train_mae = None\n",
    "        self.val_mae = None\n",
    "        self.val_prediction = None\n",
    "    \n",
    "    def fit(self, series: pd.Series):\n",
    "        df = series.to_frame().reset_index()\n",
    "        \n",
    "        # Average number of publications during a day.\n",
    "        df[\"timestamp\"] = df[\"timestamp\"].apply(datetime.datetime.fromtimestamp)\n",
    "        df[\"year_day\"] = df[\"timestamp\"].apply(lambda x: x.day_of_year)\n",
    "        series_per_day = df.groupby(\"year_day\").apply(lambda x: x[\"n_pubs\"].mean())\n",
    "        \n",
    "        # Decompose dayly trend and seasonal components.\n",
    "        result_per_day = seasonal_decompose(series_per_day, period=31, model='additive', extrapolate_trend=\"freq\")\n",
    "        self.result_per_day = result_per_day\n",
    "        \n",
    "        # Substract dayly trend and seasonal components from original series \n",
    "        to_substract = (result_per_day.trend + result_per_day.seasonal).to_frame(\"t+s\").reset_index()\n",
    "        df2 = df.merge(to_substract, on=\"year_day\")\n",
    "        df2 = df2.set_index(\"timestamp\")\n",
    "        df2[\"n_pubs\"] -= df2[\"t+s\"]\n",
    "        series_per_hour = df2[\"n_pubs\"]\n",
    "        \n",
    "        # Decompose hourly trend and seasonal components.\n",
    "        result_per_hour = seasonal_decompose(series_per_hour, period=24, model='additive', extrapolate_trend=\"freq\")\n",
    "        self.result_per_hour = result_per_hour\n",
    "        \n",
    "        # Compose dayly and hourly components.\n",
    "        df3 = (result_per_hour.trend + result_per_hour.seasonal).to_frame(\"amount\").reset_index()\n",
    "        df3[\"year_day\"] = df3[\"timestamp\"].apply(lambda x: x.day_of_year)\n",
    "        to_add = (result_per_day.trend + result_per_day.seasonal).to_frame(\"t+s\").reset_index()\n",
    "        df3 = df3.merge(to_add, on=\"year_day\")\n",
    "        df3[\"amount\"] += df3[\"t+s\"]\n",
    "        df3 = df3.set_index(\"timestamp\")\n",
    "        result = df3[\"amount\"]\n",
    "        self.result = result\n",
    "        \n",
    "        # Calculate residual.\n",
    "        series_ = series.to_frame().reset_index()\n",
    "        series_[\"timestamp\"] = series_[\"timestamp\"].apply(datetime.datetime.fromtimestamp)\n",
    "        series_ = series_.set_index(\"timestamp\")\n",
    "        series_ = series_[\"n_pubs\"]\n",
    "        residual = series_ - result\n",
    "        \n",
    "        # Train model to predict residual.\n",
    "        df_ = residual.to_frame(\"amount\").reset_index()\n",
    "        df_[\"hour\"] = df_[\"timestamp\"].apply(lambda x: x.hour)\n",
    "        df_[\"year_day\"] = df_[\"timestamp\"].apply(lambda x: x.day_of_year)\n",
    "        x = df_[[\"hour\", \"year_day\"]].to_numpy()\n",
    "        y = df_[\"amount\"].to_numpy()\n",
    "        resid_regr = RandomForestRegressor()\n",
    "        resid_regr.fit(x, y)\n",
    "        self.resid_regr = resid_regr\n",
    "        \n",
    "        # Evaluate training.\n",
    "        resid_pred = resid_regr.predict(x)\n",
    "        result_ = result.copy()\n",
    "        result_ += resid_pred\n",
    "        result_[result_ < 0] = 0\n",
    "        result_ = result_.apply(np.round)\n",
    "        self.train_mae = mean_absolute_error(result_, series_)\n",
    "\n",
    "    \n",
    "    def evaluate(self, val_series: pd.Series):\n",
    "        val_series_ = val_series.to_frame().reset_index()\n",
    "        val_series_[\"timestamp\"] = val_series_[\"timestamp\"].apply(datetime.datetime.fromtimestamp)\n",
    "        val_series_[\"timestamp\"] -= datetime.timedelta(days=365) # Hard-coded fix to enable use of previous year data.\n",
    "        val_series_ = val_series_.set_index(\"timestamp\")\n",
    "        val_series_ = val_series_[\"n_pubs\"]\n",
    "        \n",
    "        start = val_series_.index.min()\n",
    "        end = val_series_.index.max()\n",
    "        \n",
    "        # Select composed comonents data for required time range.\n",
    "        val_result_ = self.result.copy()\n",
    "        val_result_ = val_result_.loc[start: end]\n",
    "        \n",
    "        # Predict residuals.\n",
    "        df_ = val_series_.to_frame(\"amount\").reset_index()\n",
    "        df_[\"hour\"] = df_[\"timestamp\"].apply(lambda x: x.hour)\n",
    "        df_[\"year_day\"] = df_[\"timestamp\"].apply(lambda x: x.day_of_year)\n",
    "        val_x = df_[[\"hour\", \"year_day\"]].to_numpy()\n",
    "        val_resid_pred = self.resid_regr.predict(val_x)\n",
    "        val_resid_pred = pd.Series(index=df_[\"timestamp\"], data=val_resid_pred)\n",
    "        \n",
    "        # Compose result\n",
    "        val_result_ += val_resid_pred\n",
    "        val_result_[val_result_ < 0] = 0\n",
    "        val_result_ = val_result_.apply(np.round)\n",
    "        self.val_mae = mean_absolute_error(val_result_, val_series_[val_result_.index])\n",
    "        self.val_prediction = val_result_\n",
    "        \n",
    "    def save(self, dir):\n",
    "        if not os.path.exists(dir):\n",
    "            os.makedirs(dir, exist_ok=True)\n",
    "        \n",
    "        with open(os.path.join(dir, self.point_hash + \".pickle\"), \"wb\") as f:\n",
    "            pickle.dump(self, f)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/selected_data.pickle\", \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[\"train\"]\n",
    "train_points = train[\"points\"]\n",
    "train_groups = train[\"groups\"]\n",
    "\n",
    "val = data[\"val\"]\n",
    "val_points = val[\"points\"]\n",
    "val_groups = val[\"groups\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1666 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1666/1666 [25:33<00:00,  1.09it/s]\n"
     ]
    }
   ],
   "source": [
    "train_maes = []\n",
    "val_maes = []\n",
    "\n",
    "for SAMPLE_ID in tqdm(range(len(train_groups))):\n",
    "    train_point = train_points[SAMPLE_ID]\n",
    "    val_point = val_points[SAMPLE_ID]\n",
    "    assert train_point == val_point\n",
    "    train_series = train_groups[SAMPLE_ID]\n",
    "    val_series = val_groups[SAMPLE_ID]\n",
    "    model = Predictor(train_point)\n",
    "    model.fit(train_series)\n",
    "    model.evaluate(val_series)\n",
    "    train_maes.append(model.train_mae)\n",
    "    val_maes.append(model.val_mae)\n",
    "    model.save(\"./models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1666.000000\n",
       "mean        0.543562\n",
       "std         1.621069\n",
       "min         0.000000\n",
       "25%         0.108871\n",
       "50%         0.222446\n",
       "75%         0.499664\n",
       "max        41.350806\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(val_maes).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "industrial_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
